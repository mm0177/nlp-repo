{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install -q transformers==4.24.0\n\n!pip install -q gradio\n!sudo apt-get install git-lfs\n!git lfs install","metadata":{"execution":{"iopub.status.busy":"2023-10-25T16:40:41.338842Z","iopub.execute_input":"2023-10-25T16:40:41.339213Z","iopub.status.idle":"2023-10-25T16:41:32.801781Z","shell.execute_reply.started":"2023-10-25T16:40:41.339123Z","shell.execute_reply":"2023-10-25T16:41:32.800443Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n## <p style=\"background-color:#1a0a36;font-family:newtimeroman;color:#FFF9ED;font-size:140%;text-align:center;border-radius:10px 10px;\">IMPORTING LIBRARIES</p>\n#### [Top ↑](#top)","metadata":{}},{"cell_type":"code","source":"import os\nimport wandb\nimport torch\nimport pickle\nimport numpy as np\n%matplotlib inline\nimport pandas as pd\nimport gradio as gr\nimport seaborn as sns\nimport tensorflow as tf\nfrom typing import Dict\nfrom ast import literal_eval\nfrom torch.optim import AdamW\nfrom tqdm import tqdm, trange\nimport matplotlib.pyplot as plt\nfrom kaggle_secrets import UserSecretsClient\nfrom torch.nn import BCEWithLogitsLoss, BCELoss\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, f1_score, accuracy_score\nfrom transformers import XLNetForSequenceClassification, XLNetTokenizer,BertForSequenceClassification,BertTokenizer, RobertaForSequenceClassification,RobertaTokenizer\n\n# pd.set_option('Display.max_colwidth',None)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-25T16:41:32.804785Z","iopub.execute_input":"2023-10-25T16:41:32.805262Z","iopub.status.idle":"2023-10-25T16:41:51.528934Z","shell.execute_reply.started":"2023-10-25T16:41:32.805214Z","shell.execute_reply":"2023-10-25T16:41:51.527674Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn","metadata":{"execution":{"iopub.status.busy":"2023-10-25T16:41:51.53045Z","iopub.execute_input":"2023-10-25T16:41:51.531253Z","iopub.status.idle":"2023-10-25T16:41:51.536304Z","shell.execute_reply.started":"2023-10-25T16:41:51.531219Z","shell.execute_reply":"2023-10-25T16:41:51.535226Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.__version__","metadata":{"execution":{"iopub.status.busy":"2023-10-25T16:41:51.538644Z","iopub.execute_input":"2023-10-25T16:41:51.538972Z","iopub.status.idle":"2023-10-25T16:41:51.555366Z","shell.execute_reply.started":"2023-10-25T16:41:51.538931Z","shell.execute_reply":"2023-10-25T16:41:51.554353Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device_name = tf.test.gpu_device_name()\nif device_name != '/device:GPU:0':\n    raise SystemError('GPU device not found')\nprint('Found GPU at: {}'.format(device_name))","metadata":{"execution":{"iopub.status.busy":"2023-10-25T16:41:51.556719Z","iopub.execute_input":"2023-10-25T16:41:51.55714Z","iopub.status.idle":"2023-10-25T16:42:01.790948Z","shell.execute_reply.started":"2023-10-25T16:41:51.557109Z","shell.execute_reply":"2023-10-25T16:42:01.789753Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()\ntorch.cuda.get_device_name(0)","metadata":{"execution":{"iopub.status.busy":"2023-10-25T16:42:01.792364Z","iopub.execute_input":"2023-10-25T16:42:01.792904Z","iopub.status.idle":"2023-10-25T16:42:01.836578Z","shell.execute_reply.started":"2023-10-25T16:42:01.792867Z","shell.execute_reply":"2023-10-25T16:42:01.835496Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n<img src=\"https://camo.githubusercontent.com/dd842f7b0be57140e68b2ab9cb007992acd131c48284eaf6b1aca758bfea358b/68747470733a2f2f692e696d6775722e636f6d2f52557469567a482e706e67\">\n\n> I will be integrating W&B for visualizations and logging artifacts and comparisons of different models!\n> \n> [Multi Label Classification of PubMed Articles (Paper Night Presentation)]\n> https://wandb.ai/owaiskhan9515/Multi%20Label%20Classification%20of%20PubMed%20Articles%20(Paper%20Night%20Presentation)\n\n\n> \n> - To get the API key, create an account in the [website](https://wandb.ai/site) .\n> - Use secrets to use API Keys more securely ","metadata":{}},{"cell_type":"code","source":"try:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n    wandb.login(key=secret_value_0)\n    anony=None\nexcept:\n    anony = \"must\"\n    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')\n    \n    \n    \nwandb.init(project=\"Multi Label Classification of PubMed Articles (Paper Night Presentation)\",name=f\"42.Biobert-base-cased-v1.2-Run-27\")","metadata":{"execution":{"iopub.status.busy":"2023-10-25T16:42:01.837851Z","iopub.execute_input":"2023-10-25T16:42:01.838212Z","iopub.status.idle":"2023-10-25T16:42:06.322676Z","shell.execute_reply.started":"2023-10-25T16:42:01.838179Z","shell.execute_reply":"2023-10-25T16:42:06.321584Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n## <p style=\"background-color:#1a0a36;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;\"> Reading BioASQ Processed Dataset</p>\n#### [Top ↑](#top)","metadata":{}},{"cell_type":"code","source":"dataset_Name='../input/pubmed-multilabel-text-classification/PubMed Multi Label Text Classification Dataset Processed.csv'\n\ndf= pd.read_csv(dataset_Name)\ndf.head(3)","metadata":{"execution":{"iopub.status.busy":"2023-10-25T16:42:06.324768Z","iopub.execute_input":"2023-10-25T16:42:06.325129Z","iopub.status.idle":"2023-10-25T16:42:09.53265Z","shell.execute_reply.started":"2023-10-25T16:42:06.325079Z","shell.execute_reply":"2023-10-25T16:42:09.53149Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Total number of Articles extracted from Bioasq dataset are =\",len(df))","metadata":{"execution":{"iopub.status.busy":"2023-10-25T16:42:09.534025Z","iopub.execute_input":"2023-10-25T16:42:09.534336Z","iopub.status.idle":"2023-10-25T16:42:09.540904Z","shell.execute_reply.started":"2023-10-25T16:42:09.534305Z","shell.execute_reply":"2023-10-25T16:42:09.539872Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Average Article length: ', df.abstractText.str.split().str.len().mean())\nprint('Stdev Article length: ', df.abstractText.str.split().str.len().std())","metadata":{"execution":{"iopub.status.busy":"2023-10-25T16:42:09.545475Z","iopub.execute_input":"2023-10-25T16:42:09.545827Z","iopub.status.idle":"2023-10-25T16:42:12.676713Z","shell.execute_reply.started":"2023-10-25T16:42:09.545794Z","shell.execute_reply":"2023-10-25T16:42:12.675644Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cols = df.columns\ncols = list(df.columns)\nmesh_Heading_categories = cols[6:]\nnum_labels = len(mesh_Heading_categories)\nprint('Mesh Labels Root Class: \"\\n\"',mesh_Heading_categories)\nprint(\"\\n\")\nprint('Number of Labels: ' ,num_labels)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-25T16:42:12.678196Z","iopub.execute_input":"2023-10-25T16:42:12.678604Z","iopub.status.idle":"2023-10-25T16:42:12.686041Z","shell.execute_reply.started":"2023-10-25T16:42:12.678569Z","shell.execute_reply":"2023-10-25T16:42:12.684978Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Orginal Version of this Dataset contains **15,559,157 Articles** from [BioASQ Task 9A](http://participants-area.bioasq.org/datasets/).\nMore details about the format of the data and the task are available in the [Guidelines for task 9a](http://participants-area.bioasq.org/general_information/Task9a/)\n\nThis dataset which I am using currently is a preprocessed version and currently consists of a approx **50k** collection of research articles from [**PubMed**](https://pubmed.ncbi.nlm.nih.gov/) repository. Originally these documents are manually annotated by Biomedical Experts with their MeSH labels and each articles are described in terms of 10-15 MeSH labels. In this Dataset we have huge numbers of labels present as a MeSH major which is raising the issue of extremely large output space and severe label sparsity issues. To solve this Issue Dataset has been Processed and mapped to its root as Described in the Below Figure.\n![Mapped Image not Fetched](https://gitlab.com/Owaiskhan9654/Gene-Sequence-Primer/-/raw/main/Capture111.PNG)\n![Tree Structure](https://gitlab.com/Owaiskhan9654/Gene-Sequence-Primer/-/raw/main/Capture22.PNG)\n\n\n\n\nFor more information on the attributes visit [here](https://www.kaggle.com/datasets/owaiskhan9654/pubmed-multilabel-text-classification).\n\n<a id=\"3\"></a>\n## <p style=\"background-color:#1a0a36;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:5px 5px;\">DATA VISUALIZATION</p>\n#### [Top ↑](#top)\n\n#### In order to, get a full grasp of what steps should I be taking to utilizing this dataset. Let us have a look at the information in data. ","metadata":{}},{"cell_type":"code","source":"%%time\n\ncounts = []\nfor mesh_Heading_category in mesh_Heading_categories:\n    counts.append((mesh_Heading_category, df[mesh_Heading_category].sum()))\ndf_count = pd.DataFrame(counts, columns=['Root Label', 'number of Abstract'])\ndf_count","metadata":{"execution":{"iopub.status.busy":"2023-10-25T16:42:12.687399Z","iopub.execute_input":"2023-10-25T16:42:12.687812Z","iopub.status.idle":"2023-10-25T16:42:12.713137Z","shell.execute_reply.started":"2023-10-25T16:42:12.687771Z","shell.execute_reply":"2023-10-25T16:42:12.712048Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nsns.set(font_scale = 1)\nplt.figure(figsize=(16,9))\nax= sns.barplot(mesh_Heading_categories, df.iloc[:,6:].sum().values)\nplt.title(\"Each Root Class\", fontsize=22)\nplt.ylabel('Number of Articles', fontsize=18)\nplt.xlabel('Root Label ', fontsize=18)\n\n#adding the text labels\nrects = ax.patches\nlabels = df.iloc[:,6:].sum().values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom', fontsize=12)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-25T16:42:12.714542Z","iopub.execute_input":"2023-10-25T16:42:12.714833Z","iopub.status.idle":"2023-10-25T16:42:13.406692Z","shell.execute_reply.started":"2023-10-25T16:42:12.714805Z","shell.execute_reply":"2023-10-25T16:42:13.405648Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train, df_test = train_test_split(df, random_state=32, test_size=0.20, shuffle=True)\n\nprint(df_train.shape)\nprint(df_test.shape)\ndel(df)","metadata":{"execution":{"iopub.status.busy":"2023-10-25T16:42:13.408265Z","iopub.execute_input":"2023-10-25T16:42:13.408598Z","iopub.status.idle":"2023-10-25T16:42:13.436951Z","shell.execute_reply.started":"2023-10-25T16:42:13.408568Z","shell.execute_reply":"2023-10-25T16:42:13.435875Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train['one_hot_labels'] = list(df_train[mesh_Heading_categories].values)\ndf_train.head(3)","metadata":{"execution":{"iopub.status.busy":"2023-10-25T16:42:13.438209Z","iopub.execute_input":"2023-10-25T16:42:13.438494Z","iopub.status.idle":"2023-10-25T16:42:13.483096Z","shell.execute_reply.started":"2023-10-25T16:42:13.438465Z","shell.execute_reply":"2023-10-25T16:42:13.482059Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels = list(df_train.one_hot_labels.values)\nArticle_train = list(df_train.abstractText.values)","metadata":{"execution":{"iopub.status.busy":"2023-10-25T16:42:13.484399Z","iopub.execute_input":"2023-10-25T16:42:13.484703Z","iopub.status.idle":"2023-10-25T16:42:13.500837Z","shell.execute_reply.started":"2023-10-25T16:42:13.484674Z","shell.execute_reply":"2023-10-25T16:42:13.499747Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n## <p style=\"background-color:#1a0a36;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;\"> Tokenizations</p>\n#### [Top ↑](#top)","metadata":{}},{"cell_type":"code","source":"%%time\nmax_length = 128\n#tokenizer = RobertaTokenizer.from_pretrained('distilroberta-base', do_lower_case=False)  # tokenizer\ntokenizer = BertTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.2', do_lower_case=True) \n#tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=False) \n\nencodings = tokenizer.batch_encode_plus(Article_train,max_length=max_length,padding=True,truncation=True) # tokenizer's encoding method\nprint('tokenizer outputs: ', encodings.keys())\n\ninput_ids = encodings['input_ids'] # tokenized and encoded sentences\nattention_masks = encodings['attention_mask'] # attention masks","metadata":{"execution":{"iopub.status.busy":"2023-10-25T16:42:13.501974Z","iopub.execute_input":"2023-10-25T16:42:13.502288Z","iopub.status.idle":"2023-10-25T16:49:06.961557Z","shell.execute_reply.started":"2023-10-25T16:42:13.502259Z","shell.execute_reply":"2023-10-25T16:49:06.960574Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Identifying indices of 'one_hot_labels' entries that only occur once - this will allow me to stratify split our training data later\nlabel_counts = df_train.one_hot_labels.astype(str).value_counts()\none_freq = label_counts[label_counts==1].keys()\none_freq_idxs = sorted(list(df_train[df_train.one_hot_labels.astype(str).isin(one_freq)].index), reverse=True)\nprint('df label indices with only one instance: ', one_freq_idxs)","metadata":{"execution":{"iopub.status.busy":"2023-10-25T16:49:06.963129Z","iopub.execute_input":"2023-10-25T16:49:06.96356Z","iopub.status.idle":"2023-10-25T16:49:13.411065Z","shell.execute_reply.started":"2023-10-25T16:49:06.963501Z","shell.execute_reply":"2023-10-25T16:49:13.410037Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntrain_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = train_test_split(input_ids, labels, attention_masks,\n                                                            random_state=2020, test_size=0.20)\n\n# Convert all of our data into torch tensors, the required datatype for our BERT Pytorch model\ntrain_inputs = torch.tensor(train_inputs)\ntrain_labels = torch.tensor(train_labels)\ntrain_masks = torch.tensor(train_masks)\n\nvalidation_inputs = torch.tensor(validation_inputs)\nvalidation_labels = torch.tensor(validation_labels)\nvalidation_masks = torch.tensor(validation_masks)","metadata":{"execution":{"iopub.status.busy":"2023-10-25T16:49:13.412351Z","iopub.execute_input":"2023-10-25T16:49:13.412705Z","iopub.status.idle":"2023-10-25T16:49:15.131471Z","shell.execute_reply.started":"2023-10-25T16:49:13.412672Z","shell.execute_reply":"2023-10-25T16:49:15.130555Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"5\"></a>\n## <p style=\"background-color:#1a0a36;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;\"> Creating the Data Loaders</p>\n#### [Top ↑](#top)","metadata":{}},{"cell_type":"code","source":"batch_size = 64\n\n# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n# with an iterator the entire dataset does not need to be loaded into memory\n\ntrain_data =     TensorDataset(train_inputs, train_masks, train_labels,)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels,)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-10-25T16:49:15.132669Z","iopub.execute_input":"2023-10-25T16:49:15.132961Z","iopub.status.idle":"2023-10-25T16:49:15.140518Z","shell.execute_reply.started":"2023-10-25T16:49:15.132933Z","shell.execute_reply":"2023-10-25T16:49:15.139567Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(validation_dataloader,'validation_data_loader')\ntorch.save(train_dataloader,'train_data_loader')","metadata":{"execution":{"iopub.status.busy":"2023-10-25T16:49:15.141596Z","iopub.execute_input":"2023-10-25T16:49:15.141857Z","iopub.status.idle":"2023-10-25T16:49:15.313422Z","shell.execute_reply.started":"2023-10-25T16:49:15.141815Z","shell.execute_reply":"2023-10-25T16:49:15.312561Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"6\"></a>\n## <p style=\"background-color:#1a0a36;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;\"> Loading the pretrained model</p>\n#### [Top ↑](#top)","metadata":{}},{"cell_type":"code","source":"%%time\n#Tried Several Models Locally XLNet was performing Best. Note If you are changing the model then change the Tokenizer also\n# model = RobertaForSequenceClassification.from_pretrained('distilroberta-base', num_labels=num_labels)\nmodel = BertForSequenceClassification.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\", num_labels=num_labels)\n# model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=num_labels)\nmodel.cuda()\nprint('Model Pushed to Cuda for Training')","metadata":{"execution":{"iopub.status.busy":"2023-10-25T16:49:15.314626Z","iopub.execute_input":"2023-10-25T16:49:15.31493Z","iopub.status.idle":"2023-10-25T16:49:19.543423Z","shell.execute_reply.started":"2023-10-25T16:49:15.314893Z","shell.execute_reply":"2023-10-25T16:49:19.542259Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"param_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'gamma', 'beta']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.0}\n]","metadata":{"execution":{"iopub.status.busy":"2023-10-25T16:49:19.544804Z","iopub.execute_input":"2023-10-25T16:49:19.545125Z","iopub.status.idle":"2023-10-25T16:49:19.554795Z","shell.execute_reply.started":"2023-10-25T16:49:19.545092Z","shell.execute_reply":"2023-10-25T16:49:19.553583Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer = AdamW(optimizer_grouped_parameters,lr=6e-6)\n# optimizer = AdamW(model.parameters(),lr=4e-5)  # Default optimization #XL-NET","metadata":{"execution":{"iopub.status.busy":"2023-10-25T16:49:19.556191Z","iopub.execute_input":"2023-10-25T16:49:19.556618Z","iopub.status.idle":"2023-10-25T16:49:19.587147Z","shell.execute_reply.started":"2023-10-25T16:49:19.556572Z","shell.execute_reply":"2023-10-25T16:49:19.586257Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.environ['TF_FORCE_GPU_ALLOW_GROWTH']='true'","metadata":{"execution":{"iopub.status.busy":"2023-10-25T16:49:19.588476Z","iopub.execute_input":"2023-10-25T16:49:19.588832Z","iopub.status.idle":"2023-10-25T16:49:19.602165Z","shell.execute_reply.started":"2023-10-25T16:49:19.588802Z","shell.execute_reply":"2023-10-25T16:49:19.601208Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"7\"></a>\n## <p style=\"background-color:#1a0a36;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;\"> Training the model</p>\n#### [Top ↑](#top)","metadata":{}},{"cell_type":"code","source":"%%time\n\n# For Storing our loss and accuracy for plotting\ntrain_loss_set = []\nval_f1_accuracy_list,val_flat_accuracy_list,training_loss_list,epochs_list=[],[],[],[]\n\n# Number of training epochs (recommend between 5 and 10)\nepochs = 6\n\n# trange is a tqdm wrapper around the normal python range\nfor _ in trange(epochs, desc=\"Epoch \"):\n    # Training\n\n    # Set our model to training mode (as opposed to evaluation mode)\n    model.train()\n\n    # Tracking variables\n    tr_loss = 0 #running loss\n    nb_tr_examples, nb_tr_steps = 0, 0\n  \n    # Train the data for one epoch\n    for step, batch in enumerate(train_dataloader):\n        # Add batch to GPU\n        batch = tuple(t.to(device) for t in batch)\n        # Unpack the inputs from our dataloader\n        b_input_ids, b_input_mask, b_labels= batch\n        # Clear out the gradients (by default they accumulate)\n        optimizer.zero_grad()\n\n        # Forward pass for multilabel classification\n        # https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html\n        # https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html\n        # Creates a criterion that measures the Binary Cross Entropy between the target and the input probabilities\n        # Also This loss combines a Sigmoid layer and the BCELoss in one single class. This version is more numerically stable \n        # than using a plain Sigmoid followed by a BCELoss as, by combining the operations into one layer, we take advantage of the \n        # log-sum-exp trick for numerical stability.\n        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n        logits = outputs[0]\n        loss_func = BCEWithLogitsLoss()  \n        loss = loss_func(logits.view(-1,num_labels),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n        \n        train_loss_set.append(loss.item())    \n\n        # Backward pass\n        loss.backward()\n        # Update parameters and take a step using the computed gradient\n        optimizer.step()\n        # scheduler.step()\n        # Update tracking variables\n        tr_loss += loss.item()\n        nb_tr_examples += b_input_ids.size(0)\n        nb_tr_steps += 1\n\n    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n    training_loss_list.append(tr_loss/nb_tr_steps)\n\n    ###############################################################################\n\n    # Validation\n\n    # Put model in evaluation mode to evaluate loss on the validation set\n    model.eval()\n\n    # Variables to gather full output\n    logit_preds,true_labels,pred_labels,tokenized_texts = [],[],[],[]\n\n    # Predict\n    for i, batch in enumerate(validation_dataloader):\n        batch = tuple(t.to(device) for t in batch)\n        # Unpack the inputs from our dataloader\n        b_input_ids, b_input_mask, b_labels = batch\n        with torch.no_grad():\n            # Forward pass\n            outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n            b_logit_pred = outs[0]\n            pred_label = torch.sigmoid(b_logit_pred)\n\n        b_logit_pred = b_logit_pred.detach().cpu().numpy()\n        pred_label = pred_label.to('cpu').numpy()\n        b_labels = b_labels.to('cpu').numpy()\n\n    tokenized_texts.append(b_input_ids)\n    logit_preds.append(b_logit_pred)\n    true_labels.append(b_labels)\n    pred_labels.append(pred_label)\n\n    # Flatten outputs\n    pred_labels = [item for sublist in pred_labels for item in sublist]\n    true_labels = [item for sublist in true_labels for item in sublist]\n\n    # Calculate Accuracy\n    threshold = 0.50\n    pred_bools = [pl>threshold for pl in pred_labels]\n    true_bools = [tl==1 for tl in true_labels]\n    val_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')*100\n    val_flat_accuracy = accuracy_score(true_bools, pred_bools)*100\n\n    print('F1 Validation Accuracy: ', val_f1_accuracy)  \n    print('Flat Validation Accuracy: ', val_flat_accuracy)\n    print('\\n')\n    val_f1_accuracy_list.append(val_f1_accuracy)\n    val_flat_accuracy_list.append(val_flat_accuracy)\n    epochs_list.append(epochs)  \n    \n    wandb.log({\"train_loss\":tr_loss/nb_tr_steps,\"val_f1_accuracy\":val_f1_accuracy,\"val_flat_accuracy\":val_flat_accuracy,})\nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2023-10-25T16:49:19.603438Z","iopub.execute_input":"2023-10-25T16:49:19.603736Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_epochs = np.arange(1,len(training_loss_list)+1)\ndf_train_results=pd.DataFrame({'Epochs':num_epochs,'F1 Validation Accuracy':val_f1_accuracy_list,\\\n                               'Flat Validation Accuracy':val_flat_accuracy_list,'Train loss':training_loss_list})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"8\"></a>\n## <p style=\"background-color:#1a0a36;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;\"> Visualizing The results</p>\n\n#### [Top ↑](#top)","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 5));\nax.plot(num_epochs, np.array(training_loss_list) ,'bo-',label=\"Train Loss\")\nax.set_xlabel(\"Number of Epochs\")\nax.set_ylabel(\"Training Loss\")\nax.set_title(\"Training Loss vs Number of Epochs for Bert-Base\",fontsize=18)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 7));\nax.plot(num_epochs, np.array(val_f1_accuracy_list),'ro-' ,label=\"F1 Validation Accuracy\")\nax.set_xlabel(\"Number of Epochs\")\nax.set_ylabel(\"F1 Validation Accuracy\")\nax.set_title(\"F1 Validation Accuracy vs Number of Epochs for Bert-Base\",fontsize=18)\nax.set_ylim(0, 100)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 5));\nax.plot(num_epochs, np.array(val_flat_accuracy_list),'go-', label=\"Flat Validation Accuracy\")\nax.set_xlabel(\"Number of Epochs\")\nax.set_ylabel(\"Flat Validation Accuracy\")\nax.set_title(\"Flat Validation Accuracy vs Number of Epochs for for Bert-Base\",fontsize=18)\nax.set_ylim(0, 100)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test['one_hot_labels'] = list(df_test[mesh_Heading_categories].values)\ndf_test.head(3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_labels = list(df_test.one_hot_labels.values)\nArticles_test = list(df_test.abstractText.values)\ntest_mesh_categories = list(df_test.columns[6:20])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Encoding input data\ntest_encodings = tokenizer.batch_encode_plus(Articles_test,max_length=max_length,padding=True,truncation=True)\ntest_input_ids = test_encodings['input_ids']\ntest_attention_masks = test_encodings['attention_mask']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Make tensors out of data\ntest_inputs = torch.tensor(test_input_ids)\ntest_labels = torch.tensor(test_labels)\ntest_masks = torch.tensor(test_attention_masks)\n# Create test dataloader\ntest_data = TensorDataset(test_inputs, test_masks, test_labels,)# test_token_types)\ntest_sampler = SequentialSampler(test_data)\ntest_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n# Save test dataloader\ntorch.save(test_dataloader,'test_data_loader')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"9\"></a>\n## <p style=\"background-color:#1a0a36;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;\"> Evaluating the model</p>\n#### [Top ↑](#top) ","metadata":{}},{"cell_type":"code","source":"%%time\n\n# Test\n\n# Put model in evaluation mode to evaluate loss on the validation set\nmodel.eval()\n\n#track variables\nlogit_preds,true_labels,pred_labels,tokenized_texts = [],[],[],[]\n\n# Predict\nfor i, batch in enumerate(test_dataloader):\n    batch = tuple(t.to(device) for t in batch)\n    # Unpack the inputs from our dataloader\n    b_input_ids, b_input_mask, b_labels, = batch\n    with torch.no_grad():\n        # Forward pass\n        outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n        b_logit_pred = outs[0]\n        pred_label = torch.sigmoid(b_logit_pred)\n\n        b_logit_pred = b_logit_pred.detach().cpu().numpy()\n        pred_label = pred_label.to('cpu').numpy()\n        b_labels = b_labels.to('cpu').numpy()\n\n    tokenized_texts.append(b_input_ids)\n    logit_preds.append(b_logit_pred)\n    true_labels.append(b_labels)\n    pred_labels.append(pred_label)\n\n# Flatten outputs\ntokenized_texts = [item for sublist in tokenized_texts for item in sublist]\npred_labels = [item for sublist in pred_labels for item in sublist]\ntrue_labels = [item for sublist in true_labels for item in sublist]\n# Converting flattened binary values to boolean values\ntrue_bools = [tl==1 for tl in true_labels]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"10\"></a>\n## <p style=\"background-color:#1a0a36;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;\"> Classification Report</p>\n#### [Top ↑](#top)\n","metadata":{}},{"cell_type":"code","source":"pred_bools = [pl>0.50 for pl in pred_labels] #boolean output after thresholding\n# Print and save classification report\nTest_F1_Accuracy=f1_score(true_bools, pred_bools,average='micro')\nTest_Flat_Accuracy= accuracy_score(true_bools, pred_bools)\nprint('Test F1 Accuracy: ',Test_F1_Accuracy )\nprint('Test Flat Accuracy: ',Test_Flat_Accuracy,'\\n')\n\ndf_test=pd.DataFrame({'Test F1 Accuracy':Test_F1_Accuracy, 'Test Flat Accuracy':Test_Flat_Accuracy},index=[0])\n\nprint(classification_report(true_bools,pred_bools,target_names=test_mesh_categories))\nclf_report = classification_report(true_bools,pred_bools,target_names=test_mesh_categories,output_dict=True)\ndf_report=pd.DataFrame(clf_report).transpose()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_report.to_csv('Classification_Report.csv',index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained('./Multi_label_Classification_Save/')\ntokenizer.save_pretrained('./Multi_label_Classification_Save/')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"user_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"Hugging_Face_model_Push_Secret\") ##Has kept it private. Please use your own token","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Converting Labels to categorical before pushing it to Hugging Face Hub\nmodel.config.label2id= {\n\"Anatomy [A]\": 0,\n\"Organisms [B]\": 1,\n\"Diseases [C]\": 2,\n\"Chemicals and Drugs [D]\": 3,\n\"Analytical, Diagnostic and Therapeutic Techniques, and Equipment [E]\": 4,\n\"Psychiatry and Psychology [F]\": 5,\n\"Phenomena and Processes [G]\": 6,\n\"Disciplines and Occupations [H]\": 7,\n\"Anthropology, Education, Sociology, and Social Phenomena [I]\": 8,\n\"Technology, Industry, and Agriculture [J]\": 9,\n\"Information Science [L]\": 10,\n\"Named Groups [M]\": 11,\n\"Health Care [N]\": 12,\n\"Geographicals [Z]\": 13,\n}\n\n\nmodel.config.id2label={\n    \"0\": \"Anatomy [A]\",\n    \"1\": \"Organisms [B]\",\n    \"2\": \"Diseases [C]\",\n    \"3\": \"Chemicals and Drugs [D]\",\n    \"4\": \"Analytical, Diagnostic and Therapeutic Techniques, and Equipment [E]\",\n    \"5\": \"Psychiatry and Psychology [F]\",\n    \"6\": \"Phenomena and Processes [G]\",\n    \"7\": \"Disciplines and Occupations [H]\",\n    \"8\": \"Anthropology, Education, Sociology, and Social Phenomena [I]\",\n    \"9\": \"Technology, Industry, and Agriculture [J]\",\n    \"10\": \"Information Science [L]\",\n    \"11\": \"Named Groups [M]\",\n    \"12\": \"Health Care [N]\",\n    \"13\": \"Geographicals [Z]\"\n}\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.push_to_hub(repo_id='owaiskha9654/Multi-Label-Classification-of-PubMed-Articles',use_auth_token=secret_value_0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer.push_to_hub(repo_id='owaiskha9654/Multi-Label-Classification-of-PubMed-Articles',use_auth_token=secret_value_0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('owaiskha9654/Multi-Label-Classification-of-PubMed-Articles', do_lower_case=True) \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_labels=14\nmodel = BertForSequenceClassification.from_pretrained(\"owaiskha9654/Multi-Label-Classification-of-PubMed-Articles\", num_labels=num_labels)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}